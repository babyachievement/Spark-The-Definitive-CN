# 3. Spark 工具集一览



第二章中我们介绍了Spark的核心概念，如Spark结构化API中的转换和动作。这些加单的概念性模块是Apache Spark庞大的工具和库生态系统的基础。Spark由这些基础的组合-低级别API和结构化API-然后是一系列附加功能的标准库。

![0301](../images/0301.png)

Spark的库支持许多不同的任务，从图分析，机器学习到流，以及与大量计算和存储系统的集成。本章介绍Spark提供的大部分内容，包括我们尚未提及的一些API以及一些主要的库。每一节中，可以在其他本书的其他部分看到更多的细节；本章的目标是提供尽可能的概述。



本章包括以下部分：

* 使用spark-submit运行生产程序
* Dataset：结构化数据的类型安全API
* 结构化流
* 机器学习和高级分析
* RDD：Spark的低级API
* SparkR
* 第三方包生态系统

在浏览完毕后，可以跳刀本书的相应章节，找到相关特定主题的答案。



## 运行生产程序

Spark使得开发和创建大数据程序变得简单。Spark通过spark-submit（内置命令行工具）将交互式探索转换为生产应用程序变得容易。spark-submit做了一件事情：它让我们发送应用代码到一个集群然后在上面启动和执行它。提交后，应用程序将一直运行知道退出（完成任务）或碰到错误。可以用所有Spark支持的集群管理器如Standalone，Mesos和YARN执行此操作。



spark-submit提供了多种控制，有了这些控制就可以指定应用需要的资源，应用如何执行以及命令行参数。



我们还可以用任何Spark支持的开发语言写应用程序然后提交并执行。最简单的例子是在本地机器上运行应用程序。我们会通过运行一个简单的Spark附带的Scala程序展示这一点，在Spark目录中运行以下命令：

```
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master local \
  ./examples/jars/spark-examples_2.11-2.2.0.jar 10
```

这个示例程序就是计算Pi到某个小数位。这里我们告诉spark-submit我要在本地机器运行，指定了我们要运行的类和JAR，以及该类的一些命令行参数。



我们也可以使用下边的命令运行这个程序的Python版本：

```Python
./bin/spark-submit \
  --master local \
  ./examples/src/main/python/pi.py 10
```

通过修改spark-submit的master参数，我们可以将相同的程序提交到给运行Spark的独立集群管理器，Mesos或YARN。



spark-submit在运行本书中打包的许多实例时将派上用场。在本章接下来的内容中，我们看看一些在Spark介绍中尚未看到的API的例子。



## Dataset：类型安全的结构化API

我们将介绍的第一个API是Spark结构化API的一个类型安全版本缴Dataset，用于在Java和Scala中写静态类型的代码。Dataset API在Python和R中是不可用的，因为这些语言是动态类型的。



回想一下，我们在前一章中看到的DataFrame是Row类型的对象分布式集合，可以容纳各种类型的表格数据。Dataset API使用户能够将Java/Scala类分配给DatgaFrame中的类并将其作为类型化对象的集合进行操作，类似于Java ArrayList或Scala Seq。Dataset上可用的API是类型安全的，也就是说你不能偶然地将数据集中的对象视为与最初放入的类不同的类的对象。这使得Dataset对于写大应用程序特别有吸引力，有了它多个软件工程师必须通过定义好的接口进行交互。



Dataset类使用包含的对象类型进行参数化：Java中的Dataset<T>和Scala中的Dataset[T]。比如，Dataset[Person]会保证只能包含Person类的对象。从Spark2.0开始，支持的类型是遵循Java中的JavaBean模式和Scala中的case类。这些类型是收到限制的，因为Spark需要能够自动分析类型T并为Dataset中的表格数据创建适当的模式。



关于Dataset的一个好处，只有在需要或想要时才能使用它们。比如，在下边的例子中，我们会定义我们的数据类型并通过任意map和过滤功能操作它。在执行了我们的操作之后，Spark可以自动将其重新转换为DataFrame，并且我们可以通过使用Spark包含的数百个函数来进一步操作它。这使得降级到低级别，在需要时执行类型安全的的代码，和升级到SQL进行更快速的分析变得简单。下边的小例子展示了如何使用类型安全的函数和类DataFrame的SQL表达式快速写业务逻辑：

```Scala
// in Scala
case class Flight(DEST_COUNTRY_NAME: String,
                  ORIGIN_COUNTRY_NAME: String,
                  count: BigInt)
val flightsDF = spark.read
  .parquet("/data/flight-data/parquet/2010-summary.parquet/")
val flights = flightsDF.as[Flight]
```

最后一个优点是，当在Dataset上调用collect或take时，它将在Dataset中收集正确类型才能够的对象，而不是DataFrame Rows。这样可以轻松获得类型安全且可以在分布式和本地中不需要修改代码安全地执行操作：

```Scala
// in Scala
flights
  .filter(flight_row => flight_row.ORIGIN_COUNTRY_NAME != "Canada")
  .map(flight_row => flight_row)
  .take(5)

flights
  .take(5)
  .filter(flight_row => flight_row.ORIGIN_COUNTRY_NAME != "Canada")
  .map(fr => Flight(fr.DEST_COUNTRY_NAME, fr.ORIGIN_COUNTRY_NAME, fr.count + 5))
```

我们在第11章中深入的介绍Dataset。



## 结构化Streaming

结构化Streaming是在Spark2.2中变成生产级别的流式处理高级API。使用结构化流式处理，可以用流式的方式执行一些在批处理中用到的Spark 结构化API操作。这可以减少延迟并运行增量处理。关于结构化流的最好的事是它让我们不用修改代码就能高速快速地从流流中提取值。它还是概念化变得容易，以为我们可以将批处理作业作为一个方式原型化它，然后就可以将其转换为流作业。所有这些工作的方式是增量地处理数据。



我们通过一个简单的例子来看看使用结构化Streaming是多么简单。我们使用retail-data 数据集，它有特殊的日期和时间供我们可用。我使用按天划分的文件集，每个文件代表一天的数据。



我们使用这种格式模拟被不同进程按照一致和规则的方式产生的数据。这是零售数据，因此想象这些是零售店正在生产的，并被发送到一个可以被我妈的流作业读取的位置。



分享数据的一个样本以便于你能够参考是值得的：

```
InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country
536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,2010-12-01 08:26:00,2.55,17...
536365,71053,WHITE METAL LANTERN,6,2010-12-01 08:26:00,3.39,17850.0,United Kin...
536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,2010-12-01 08:26:00,2.75,17850...
```



因此，我们首先将数据视为静态数据集进行分析然后创建一个DataFrame。我们也从这个静态数据集中创建一个模式（流中有多种我们会接触到的模式推断方式）:

```Scala
// in Scala
val staticDataFrame = spark.read.format("csv")
  .option("header", "true")
  .option("inferSchema", "true")
  .load("/data/retail-data/by-day/*.csv")

staticDataFrame.createOrReplaceTempView("retail_data")
val staticSchema = staticDataFrame.schema
```

```Python
# in Python
staticDataFrame = spark.read.format("csv")\
  .option("header", "true")\
  .option("inferSchema", "true")\
  .load("/data/retail-data/by-day/*.csv")

staticDataFrame.createOrReplaceTempView("retail_data")
staticSchema = staticDataFrame.schema
```

因为我们在使用时序数据，值得一提的是我们如何分组和聚合我们的数据。在本例中，我们将查看特定客户（由CustomerId识别）进行大量购买的销售时间。比如，我们增加一个总花费列，看一下一个客户在哪天消费了最多。



window函数在聚合中将包含每天的所有数据。它只是我们数据中时间序列列上的一个窗口。这对于操作日期和时间错是非常有用的工具，因为我们可以以更人性化的形式（通过间隔）指定我们的需求，Spark为我们将这些组合在一起：

```Scala
// in Scala
import org.apache.spark.sql.functions.{window, column, desc, col}
staticDataFrame
  .selectExpr(
    "CustomerId",
    "(UnitPrice * Quantity) as total_cost",
    "InvoiceDate")
  .groupBy(
    col("CustomerId"), window(col("InvoiceDate"), "1 day"))
  .sum("total_cost")
  .show(5)
```

```Python
# in Python
from pyspark.sql.functions import window, column, desc, col
staticDataFrame\
  .selectExpr(
    "CustomerId",
    "(UnitPrice * Quantity) as total_cost",
    "InvoiceDate")\
  .groupBy(
    col("CustomerId"), window(col("InvoiceDate"), "1 day"))\
  .sum("total_cost")\
  .show(5)
```

值得注意的是可以将其以SQL代码运行，正如我们在前一章中看到的。



下边是你看到的输出例子：

```
+----------+--------------------+------------------+
|CustomerId|              window|   sum(total_cost)|
+----------+--------------------+------------------+
|   17450.0|[2011-09-20 00:00...|          71601.44|
...
|      null|[2011-12-08 00:00...|31975.590000000007|
+----------+--------------------+------------------+
```

null值表示我们没有CustomerId。



这是静态DataFrame版本；如果你熟悉这些语法就不会有什么惊讶的了。



因为你可能是在本地模式中运行的，设置shuffle分区数量为一个合适的值是一个好的实践。这个配置指定shuffle后创建的分区的数量。默认，这个值是200，但是可能机器上没有那么多executor，减到5是值得的。我们在第二章中做了相同的操作，所以如果不记得为什么它很重要，可以随时回顾一下。

```Scala
spark.conf.set("spark.sql.shuffle.partitions", "5")
```

现在我们看到它如何起作用，我们看一下流式的代码！你会注意到非常少的代码变化。最大的变化是我们使用readStream而不是read，此外还会注意到maxFilesPerTrigger参数，用来指定一次读取的文件数量。这使得我们的演示更“streaming”，在生产场景中，这个能会被省略。

```Scala
val streamingDataFrame = spark.readStream
    .schema(staticSchema)
    .option("maxFilesPerTrigger", 1)
    .format("csv")
    .option("header", "true")
    .load("/data/retail-data/by-day/*.csv")
```

```Python
# in Python
streamingDataFrame = spark.readStream\
    .schema(staticSchema)\
    .option("maxFilesPerTrigger", 1)\
    .format("csv")\
    .option("header", "true")\
    .load("/data/retail-data/by-day/*.csv")
```

现在我们可以看看我们的DataFrame是不是流：

```Scala
streamingDataFrame.isStreaming // returns true
```

我们建立行业之前的DataFrame操作相同的业务逻辑。我们执行一个求和：

```Scala
// in Scala
val purchaseByCustomerPerHour = streamingDataFrame
  .selectExpr(
    "CustomerId",
    "(UnitPrice * Quantity) as total_cost",
    "InvoiceDate")
  .groupBy(
    $"CustomerId", window($"InvoiceDate", "1 day"))
  .sum("total_cost")
```

```Python
# in Python
purchaseByCustomerPerHour = streamingDataFrame\
  .selectExpr(
    "CustomerId",
    "(UnitPrice * Quantity) as total_cost",
    "InvoiceDate")\
  .groupBy(
    col("CustomerId"), window(col("InvoiceDate"), "1 day"))\
  .sum("total_cost")
```

这是一个惰性操作，所以我们需要调用一个streaming动作来启动这个数据流的执行。



流式动作与我们传统的静态操作有点不同，因为我们将在某处填充数据，而不是只调用count之类的（无论如何在流上都没有任何意义）。我们将使用的动作将输出到一个内存表中，每次触发后将会更新。这种情况下，每个触发器是基于单个文集上的（我们设置的读参数）。Spark会修改内存表中的数据以便我们总能获取我们之前聚合中指定的最大值：

```Scala
// in Scala
purchaseByCustomerPerHour.writeStream
    .format("memory") // memory = store in-memory table
    .queryName("customer_purchases") // the name of the in-memory table
    .outputMode("complete") // complete = all the counts should be in the table
    .start()
```

```Python
# in Python
purchaseByCustomerPerHour.writeStream\
    .format("memory")\
    .queryName("customer_purchases")\
    .outputMode("complete")\
    .start()
```

我们启动流后，可以对它进行查询来调试结果是什么样的如果我们之前将结果输出到一个生产sink中：

```Scala
// in Scala
spark.sql("""
  SELECT *
  FROM customer_purchases
  ORDER BY `sum(total_cost)` DESC
  """)
  .show(5)
```

```Python
# in Python
spark.sql("""
  SELECT *
  FROM customer_purchases
  ORDER BY `sum(total_cost)` DESC
  """)\
  .show(5)
```

你会注意当读入更多数据时表格的组成会发生变化！对于每个文件，结果可能会也可能不会根据数据变化。自然，因为我们分组客户，我们希望看到顶部的客户购买量随着时间的推移而增加（并且会持续一段时间！）。另一个可用的将结果写出的选项是console:

```Scala
purchaseByCustomerPerHour.writeStream
    .format("console")
    .queryName("customer_purchases_2")
    .outputMode("complete")
    .start()
```

你不需要在生产中使用这些流方法中的任何一个，但他们对于演示结构化流的能力提供了便利。注意这个window是构建在时间时间上的，而不是Spark处理数据的时间。这是结构化流已经解决的Spark Streaming的缺点之一。我们将在第五部分更深入地介绍流。



## 机器学习和高级分析

Spark另一个流行的方面是它使用内置的机器学习库MLlib执行大规模机器学习的能力。MLlib支持预处理，整理和模型训练，对数据进行大规模预测。你时甚至可以在 Strucutred Streaming中使用MLlib中训练的模型。Spark提供了一个复杂的机器学习API用于执行各种机器学习任务，从分类到回归，聚合到深度学习。为了演示这个功能，我们会子啊我们的数据上使用*k-means*执行一些基础的聚合。

​	*k*-means是一个聚合算法，k中心在数据中是随机赋值的。然后将最接近该店的点分配给一个类，并计算分配点的中心。该中心点称为质心。然后，我们将靠近该质心的点标记到质心的类，并将质心移动到集群的新中心。我们有限次迭代重复这个过程或指导收敛（中心点停止变化）。



Spark开箱包含许多预处理方法。要演示这个鞋方法，我们从原始数据开始，在将数据转换为合适的格式前构建转换，此时我们可以实际训练我们的莫模型然后提供预测：

```Scala
staticDataFrame.printSchema()
```

```
root
 |-- InvoiceNo: string (nullable = true)
 |-- StockCode: string (nullable = true)
 |-- Description: string (nullable = true)
 |-- Quantity: integer (nullable = true)
 |-- InvoiceDate: timestamp (nullable = true)
 |-- UnitPrice: double (nullable = true)
 |-- CustomerID: double (nullable = true)
 |-- Country: string (nullable = true)
```



​	MLlib中的机器学习算法需要数据表示为数字。我们当前的数据多种不同的类型，包括时间戳，整数和字符串。因此我们需要将数据转换为数字格式。在这个例子中，我们将使用多个DataFrame转换操作我们的日期数据：

```Scala
// in Scala
import org.apache.spark.sql.functions.date_format
val preppedDataFrame = staticDataFrame
  .na.fill(0)
  .withColumn("day_of_week", date_format($"InvoiceDate", "EEEE"))
  .coalesce(5)
```

```Python
# in Python
from pyspark.sql.functions import date_format, col
preppedDataFrame = staticDataFrame\
  .na.fill(0)\
  .withColumn("day_of_week", date_format(col("InvoiceDate"), "EEEE"))\
  .coalesce(5)
```

我们还将需要将数据分给成训练和测试集。在这个例子中，我们将按照发生某次购买的日期手动执行此操作；但是，我们也可以使用MLlib的转换API通过拆分或交叉验证创建训练和测试数据集（这些主题在第6部分详细介绍）：

```Scala
// in Scala
val trainDataFrame = preppedDataFrame
  .where("InvoiceDate < '2011-07-01'")
val testDataFrame = preppedDataFrame
  .where("InvoiceDate >= '2011-07-01'")
```

```Python
# in Python
trainDataFrame = preppedDataFrame\
  .where("InvoiceDate < '2011-07-01'")
testDataFrame = preppedDataFrame\
  .where("InvoiceDate >= '2011-07-01'")
```

